{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13895246,"sourceType":"datasetVersion","datasetId":8852687}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Environment & warnings ---\nimport os, warnings, torch, random, re\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom datasets import load_dataset, concatenate_datasets, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\n\n# -----------------------------\n# 1) Load CSV\n# -----------------------------\ndataset = load_dataset(\"csv\", data_files={\"train\": \"/kaggle/input/latest/LDataset.csv\"})[\"train\"]\n\n# -----------------------------\n# 2) Helpers (Roman normalization + length filter)\n# -----------------------------\ndef normalize_roman(s):\n    if s is None:\n        return \"\"\n    s = str(s).lower().strip()\n    s = re.sub(r\"[^a-z\\s']\", \" \", s)  # keep letters, spaces, apostrophes\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef ok_len(ex):\n    src_len = len(str(ex[\"source\"]).split())\n    tgt_len = len(str(ex[\"target\"]).split())\n    return (3 <= src_len <= 40) and (1 <= tgt_len <= 64)\n\n# -----------------------------\n# 3) Expand into source-target pairs with language tags\n# -----------------------------\ndef build_pairs(batch):\n    sources, targets = [], []\n    for g, r, h, e in zip(batch[\"Garhwali\"], batch[\"RomanGarhwali\"], batch[\"Hindi\"], batch[\"English\"]):\n        g = \"\" if g is None else str(g).strip()\n        r = normalize_roman(r)\n        h = \"\" if h is None else str(h).strip()\n        e = \"\" if e is None else str(e).strip()\n\n        sources.append(f\"<gar> translate to <eng>: {g}\"); targets.append(e)\n        sources.append(f\"<gar> translate to <hin>: {g}\"); targets.append(h)\n        sources.append(f\"<rgar> translate to <eng>: {r}\"); targets.append(e)\n        sources.append(f\"<rgar> translate to <hin>: {r}\"); targets.append(h)\n    return {\"source\": sources, \"target\": targets}\n\nexpanded = dataset.map(\n    build_pairs, batched=True,\n    remove_columns=[\"Garhwali\",\"RomanGarhwali\",\"Hindi\",\"English\"]\n)\n\n# -----------------------------\n# 4) Clean: drop empty targets, dedup sources, filter lengths\n# -----------------------------\ndef has_target(example):\n    t = example[\"target\"]\n    return t is not None and isinstance(t, str) and len(t.strip()) > 0\n\nclean = expanded.filter(has_target)\n\n# Deduplicate by source using a filter\nseen = set()\ndef dedup(example):\n    src = example[\"source\"]\n    if src in seen:\n        return False\n    seen.add(src)\n    return True\n\nclean = clean.filter(dedup)\nclean = clean.filter(ok_len)\n\n# -----------------------------\n# 5) Split\n# -----------------------------\nsplit = clean.train_test_split(test_size=0.1, seed=42)\ntrain_raw, eval_raw = split[\"train\"], split[\"test\"]\n\n# -----------------------------\n# 6) Tokenizer & Model (IndicBART) + add special tokens\n# -----------------------------\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indicbart\")\n\nspecial_tokens = {\"additional_special_tokens\": [\"<gar>\", \"<rgar>\", \"<hin>\", \"<eng>\"]}\nnum_added = tokenizer.add_special_tokens(special_tokens)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indicbart\")\nif num_added > 0:\n    model.resize_token_embeddings(len(tokenizer))\n\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.forced_bos_token_id = tokenizer.bos_token_id\n\n# -----------------------------\n# 7) Preprocess with masking\n# -----------------------------\nPAD_ID = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n\ndef preprocess_function(examples):\n    sources = [str(s) for s in examples[\"source\"]]\n    targets = [str(t) if t is not None and len(str(t).strip()) > 0 else \"<unk>\" for t in examples[\"target\"]]\n\n    inputs = tokenizer(sources, max_length=96, truncation=True, padding=\"max_length\")\n    labels_tok = tokenizer(targets, max_length=96, truncation=True, padding=\"max_length\")\n\n    label_ids = labels_tok[\"input_ids\"]\n    masked_labels = [[(tid if tid != PAD_ID else -100) for tid in seq] for seq in label_ids]\n    inputs[\"labels\"] = masked_labels\n    return inputs\n\n# -----------------------------\n# 8) Oversample Hindi and Roman\n# -----------------------------\nhindi_pairs = train_raw.filter(lambda x: \"<hin>\" in x[\"source\"])\nroman_pairs = train_raw.filter(lambda x: \"<rgar>\" in x[\"source\"])\n\nbalanced_train = concatenate_datasets([\n    train_raw,\n    hindi_pairs,\n    roman_pairs, roman_pairs\n]).shuffle(seed=42)\n\n# -----------------------------\n# 9) Tokenize datasets\n# -----------------------------\ntokenized_train = balanced_train.map(preprocess_function, batched=True, remove_columns=[\"source\",\"target\"])\ntokenized_eval = eval_raw.map(preprocess_function, batched=True, remove_columns=[\"source\",\"target\"])\n\ndef has_any_label(ex):\n    return any(t != -100 for t in ex[\"labels\"])\ntokenized_eval = tokenized_eval.filter(has_any_label)\n\n# -----------------------------\n# 10) Data collator\n# -----------------------------\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n\n# -----------------------------\n# 11) Training arguments\n# -----------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=6,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_strategy=\"epoch\",         # use 'evaluation_strategy' if your transformers supports it\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    fp16=False,\n    max_grad_norm=1.0,\n    learning_rate=3e-5,\n    warmup_steps=500,\n    label_smoothing_factor=0.1,\n    report_to=\"none\",\n    remove_unused_columns=True\n)\n\n# -----------------------------\n# 12) Trainer\n# -----------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_eval,\n    data_collator=data_collator\n)\n\n# -----------------------------\n# 13) Train\n# -----------------------------\nprint(\"üöÄ Starting training...\")\ntrain_out = trainer.train()\nprint(\"‚úÖ Training complete.\")\n\n# -----------------------------\n# 14) Inference helper\n# -----------------------------\ndef translate(text, beams=6):\n    device = model.device\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    inputs.pop(\"token_type_ids\", None)\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=80,\n        num_beams=beams,\n        do_sample=False,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.05,\n        length_penalty=1.0,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n\n# -----------------------------\n# 15) Sanity check loop\n# -----------------------------\ndef sanity_check(dataset, n=5):\n    samples = random.sample(range(len(dataset)), n)\n    for idx in samples:\n        ex = dataset[idx]\n        src = ex[\"source\"]\n        tgt = ex[\"target\"]\n        pred = translate(src)\n        print(\"üìù Source:\", src)\n        print(\"üéØ Target:\", tgt)\n        print(\"ü§ñ Model Output:\", pred)\n        print(\"-\" * 60)\n\nsanity_check(eval_raw, n=5)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:56:17.726451Z","iopub.execute_input":"2025-11-30T06:56:17.726729Z","iopub.status.idle":"2025-11-30T09:50:53.174709Z","shell.execute_reply.started":"2025-11-30T06:56:17.726690Z","shell.execute_reply":"2025-11-30T09:50:53.174066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------------\n# 16) Save model and tokenizer to output directory\n# -----------------------------\nSAVE_PATH = \"/kaggle/working/translation_model\"\n\ntokenizer.save_pretrained(SAVE_PATH)\nmodel.save_pretrained(SAVE_PATH)\n\nprint(f\"‚úÖ Model and tokenizer saved to {SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:51:18.002883Z","iopub.execute_input":"2025-11-30T09:51:18.003153Z","iopub.status.idle":"2025-11-30T09:51:19.681946Z","shell.execute_reply.started":"2025-11-30T09:51:18.003120Z","shell.execute_reply":"2025-11-30T09:51:19.681158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Garhwali ‚Üí Hindi\nprint(translate(\"<gar> translate to <hin>: ‡§Æ‡•ç‡§Ø‡§æ‡§∞ ‡§®‡•å‡§Ç ‡§∏‡§æ‡§ï‡•ç‡§∑‡•Ä ‡§õ\"))\n\n# Example: Garhwali ‚Üí English\nprint(translate(\"<gar> translate to <eng>: ‡§Æ‡•ç‡§Ø‡§æ‡§∞ ‡§®‡•å‡§Ç ‡§∏‡§æ‡§ï‡•ç‡§∑‡•Ä ‡§õ\"))\n\n# Example: Roman Garhwali ‚Üí Hindi\nprint(translate(\"<rgar> translate to <hin>: myar naun sakshi ch\"))\n\n# Example: Roman Garhwali ‚Üí English\nprint(translate(\"<rgar> translate to <eng>: myar naun sakshi ch\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:51:37.751906Z","iopub.execute_input":"2025-11-30T09:51:37.752452Z","iopub.status.idle":"2025-11-30T09:51:38.064096Z","shell.execute_reply.started":"2025-11-30T09:51:37.752426Z","shell.execute_reply":"2025-11-30T09:51:38.063504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(translate(\"<gar> translate to <hin>: ‡§Æ‡•Ä ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ‡§®‡•Ç ‡§õ‡•å‡§Ç‡•§\"))\nprint(translate(\"<gar> translate to <eng>: ‡§§‡•Ç ‡§Ü‡§ú ‡§≠‡§ø‡§£‡§∏‡§æ‡§∞‡§ø ‡§≠‡§æ‡§§ ‡§ö‡§¢‡§æ‡§Ø‡§≤‡§ø?\"))\nprint(translate(\"<rgar> translate to <hin>: mi kitaab paddu chon\"))\nprint(translate(\"<rgar> translate to <eng>: mi kitaab paddu chon\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:51:44.548974Z","iopub.execute_input":"2025-11-30T09:51:44.549594Z","iopub.status.idle":"2025-11-30T09:51:44.908234Z","shell.execute_reply.started":"2025-11-30T09:51:44.549571Z","shell.execute_reply":"2025-11-30T09:51:44.907605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save final model + tokenizer into a folder\ntrainer.save_model(\"./trained_model\")\ntokenizer.save_pretrained(\"./trained_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:51:48.120368Z","iopub.execute_input":"2025-11-30T09:51:48.120624Z","iopub.status.idle":"2025-11-30T09:51:49.773665Z","shell.execute_reply.started":"2025-11-30T09:51:48.120607Z","shell.execute_reply":"2025-11-30T09:51:49.772975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Move the trained model folder into Kaggle's output directory\nshutil.move(\"./trained_model\", \"/kaggle/outputs/trained_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:52:03.608711Z","iopub.execute_input":"2025-11-30T09:52:03.609342Z","iopub.status.idle":"2025-11-30T09:52:04.355208Z","shell.execute_reply.started":"2025-11-30T09:52:03.609317Z","shell.execute_reply":"2025-11-30T09:52:04.354627Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/translation_model.zip /kaggle/working/translation_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:54:27.829628Z","iopub.execute_input":"2025-11-30T09:54:27.830265Z","iopub.status.idle":"2025-11-30T09:55:17.868968Z","shell.execute_reply.started":"2025-11-30T09:54:27.830233Z","shell.execute_reply":"2025-11-30T09:55:17.868192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/translation_model.zip /kaggle/working/translation_model\nimport shutil\nshutil.move(\"/kaggle/working/translation_model.zip\", \"/kaggle/outputs/translation_model.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:55:17.870515Z","iopub.execute_input":"2025-11-30T09:55:17.870814Z","iopub.status.idle":"2025-11-30T09:56:08.502637Z","shell.execute_reply.started":"2025-11-30T09:55:17.870788Z","shell.execute_reply":"2025-11-30T09:56:08.501769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/outputs/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:56:15.502869Z","iopub.execute_input":"2025-11-30T09:56:15.503502Z","iopub.status.idle":"2025-11-30T09:56:15.670482Z","shell.execute_reply.started":"2025-11-30T09:56:15.503469Z","shell.execute_reply":"2025-11-30T09:56:15.669339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/translation_model.zip /kaggle/working/translation_model\n!mv /kaggle/working/translation_model.zip /kaggle/outputs/translation_model.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:56:36.698288Z","iopub.execute_input":"2025-11-30T09:56:36.698578Z","iopub.status.idle":"2025-11-30T09:57:28.020713Z","shell.execute_reply.started":"2025-11-30T09:56:36.698552Z","shell.execute_reply":"2025-11-30T09:57:28.019614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\ndef translate_ui(text, src_token, tgt_token):\n    prompt = f\"{src_token} translate to {tgt_token}: {text}\"\n    try:\n        return translate(prompt)\n    except Exception as e:\n        return \"Error\"\n\nwith gr.Blocks(theme=gr.themes.Base()) as demo:\n    gr.Markdown(\n        \"<h1 style='text-align: center; color: #FFD700;'>üåê Garhwali/Hindi/English Translator</h1>\",\n    )\n\n    with gr.Row():\n        src_dropdown = gr.Dropdown(\n            choices=[\"Garhwali\"],\n            label=\"Source\",\n            value=\"Garhwali\"\n        )\n        gr.Markdown(\"<h2 style='text-align: center;'>‚áÑ</h2>\")\n        tgt_dropdown = gr.Dropdown(\n            choices=[\"Hindi\", \"English\"],\n            label=\"Target\",\n            value=\"Hindi\"\n        )\n\n    with gr.Row():\n        input_text = gr.Textbox(lines=8, label=\"Input Text\", placeholder=\"Enter text here...\")\n        output_text = gr.Textbox(lines=8, label=\"Translated Output\")\n\n    with gr.Row():\n        translate_btn = gr.Button(\"Translate\", variant=\"primary\")\n\n    def on_translate(text, src_ui, tgt_ui):\n        # Map UI labels to internal tokens\n        src_token = \"<rgar>\" if src_ui == \"Garhwali\" else \"<gar>\"\n        tgt_token = \"<hin>\" if tgt_ui == \"Hindi\" else \"<eng>\"\n        return translate_ui(text, src_token, tgt_token)\n\n    translate_btn.click(\n        on_translate,\n        inputs=[input_text, src_dropdown, tgt_dropdown],\n        outputs=output_text\n    )\n\ndemo.launch(share=True, inline=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:57:50.723153Z","iopub.execute_input":"2025-11-30T09:57:50.723493Z","iopub.status.idle":"2025-11-30T09:57:55.232252Z","shell.execute_reply.started":"2025-11-30T09:57:50.723462Z","shell.execute_reply":"2025-11-30T09:57:55.231632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SAVE_PATH = \"./translation_model\"\ntokenizer.save_pretrained(SAVE_PATH)\nmodel.save_pretrained(SAVE_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:04:30.521893Z","iopub.execute_input":"2025-11-30T10:04:30.522179Z","iopub.status.idle":"2025-11-30T10:04:32.352999Z","shell.execute_reply.started":"2025-11-30T10:04:30.522157Z","shell.execute_reply":"2025-11-30T10:04:32.352163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/translation_model.zip ./translation_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:01:36.891244Z","iopub.execute_input":"2025-11-30T10:01:36.891551Z","iopub.status.idle":"2025-11-30T10:02:27.014979Z","shell.execute_reply.started":"2025-11-30T10:01:36.891530Z","shell.execute_reply":"2025-11-30T10:02:27.014249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}